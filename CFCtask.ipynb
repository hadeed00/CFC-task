{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled8.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install soupsieve\n",
        "!pip install urllib3\n",
        "!pip install chardet\n",
        "!pip install idna\n",
        "!pip install requests\n",
        "!pip install beautifulsoup4\n",
        "!pip install certifi"
      ],
      "metadata": {
        "id": "J62OPAkh0Q9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAXxoJL_0FN6"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from bs4.element import Comment\n",
        "import json\n",
        "import string\n",
        "\n",
        "#### Importing all necessary libraries needed above.\n",
        "\n",
        "\n",
        "\n",
        "def tag_visibility(element): #Function to read over all the text and return false for every bit of text that falls under a specified list.\n",
        "    remove = ['head', 'title', 'meta', 'script', 'style', '[document]'] #list of items to remove from list.\n",
        "    if element.parent.name in remove: \n",
        "        return False\n",
        "    if isinstance(element, Comment):\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "# Return the visible text from the soup of a page, using the filters outlines in tag is visible 'page': Page from the result of a requests.get() operation\n",
        "def visibility(page):\n",
        "    soup = BeautifulSoup(page.text, 'html.parser') #Using BeautifulSoup to find all the text in the page.\n",
        "    text = soup.findAll(text=True) #Finding out which text is visible to users and ensuring to only store that as text.\n",
        "    visible_text = filter(tag_visibility, text) #Removes all the text that we have blacklisted in the function above.\n",
        "    return u' '.join(t.strip() for t in visible_text) #\n",
        "\n",
        "\n",
        "# and return a dictionary of word occurances where the keys are words, and the values the respective number of occurances.\n",
        "# 'link': string, the URL of the page to parse for (visible) word frequency\n",
        "# ? Could add options to include hidden words in the evaluation, or blacklist certain tags etc.\n",
        "\n",
        "def calculate_freq():\n",
        "    page = requests.get(privacy_policy_url) #Getting and storing the data from the privacy policy page.\n",
        "\n",
        "    text_from_page = visibility(page).encode('ascii', 'ignore').decode().translate(str.maketrans('', '', string.punctuation)) #Cleaning out all punction and symbols to just have the text.\n",
        "    words_from_page = text_from_page.split(' ') #Splits up the text using spaces.\n",
        "\n",
        "    word_count = {} #Create a dictionary to track all words and counts. This will be returned at the end.\n",
        "    for word in words_from_page: #Create a loop to go over every word in the page.\n",
        "        if (word == ''): \n",
        "            continue #Skip empty strings.\n",
        "        \n",
        "        word = word.upper() #.upper used here to turn everything uppercase so it's not case sensitive.\n",
        "\n",
        "        if word in word_count: #Adjusting values in the dictionary or creating a new entry. \n",
        "            word_count[word] += 1 \n",
        "        else:\n",
        "            word_count[word] = 1\n",
        "    return word_count\n",
        "\n",
        "cfcurl = 'https://www.cfcunderwriting.com'     # The CFC url given in the specification that needs to be scraped.\n",
        "privacy_policy_url = \"\" #Setting up the privacy policy url variable as a String, the data will be added seperately.\n",
        "\n",
        "def external_check(url): #This function is used to find if a link comes internally or externally based on the url given.\n",
        "    if (not url): #Returns false for empty strings.\n",
        "        return False\n",
        "\n",
        "    # Local files, denoted by local reference '/' are considered not external, as are fragment links '#' \n",
        "    # ? (fragments wont link to resources, but this may have other use cases if extended)\n",
        "    if (url[0] == '/'):\n",
        "        return False\n",
        "    if(url[0] == '#'):\n",
        "        return False\n",
        "\n",
        "    return True\n",
        "\n",
        "def find_attributes(tag, attribute):\n",
        "    page = requests.get(cfcurl)\n",
        "    soup = BeautifulSoup(page.text, 'html.parser') #Identifying all external links by using the beautifulsoup function\n",
        "\n",
        "    allAtributes = []\n",
        "    \n",
        "    for item in soup.findAll(tag): #Creating a loop to go over every item in the soup with the specified tag.\n",
        "        try: \n",
        "            location = item[attribute] #Attribute is for the tag which links to a resource.\n",
        "            allAtributes.append(location) #Appending values to the array.\n",
        "        except KeyError:\n",
        "            # Item doesnt have the desired attribute\n",
        "            pass\n",
        "\n",
        "    return allAtributes\n",
        "\n",
        "def get_external_resources():\n",
        "    \n",
        "\n",
        "    resources = []\n",
        "    resources += find_attributes('img', 'src') #This doesn't return any values but is included for the specification.\n",
        "    resources += find_attributes('link', 'href') #These tags were chosen based on the specification.\n",
        "    resources += find_attributes('script', 'src') #Essentially getting every link for each tag.\n",
        "\n",
        "    external_resources = [cfcurl for cfcurl in resources if external_check(cfcurl)] #Go over every link and remove any local links.\n",
        "    return external_resources\n",
        "\n",
        "def enumeration():\n",
        "\n",
        "    \n",
        "    page = requests.get(cfcurl)# Gets the page link\n",
        "    soup = BeautifulSoup(page.text, 'html.parser')\n",
        "\n",
        "    allLinks = [] \n",
        "\n",
        "    for link in soup.findAll('a'):\n",
        "        try:\n",
        "            # Get the links destination, then append to list of all links with it's text content\n",
        "            link_destination = link['href']\n",
        "            allLinks.append((link_destination, link.find(text=True))) #Adding All neccessary links to the array with a matching text.\n",
        "\n",
        "        except KeyError:\n",
        "            pass\n",
        "\n",
        "    return allLinks\n",
        "\n",
        "\n",
        "def search_link_text(hyperlinks, targeted_text): #Finds the exact location of privacy policy by using the key words as the targeted_text\n",
        "    for (hyperlink, link_text) in hyperlinks:\n",
        "        try:\n",
        "            if link_text.lower() == targeted_text.lower():\n",
        "                return hyperlink\n",
        "        except AttributeError:\n",
        "            pass\n",
        "    return None\n",
        "\n",
        "\n",
        "def save_json_file(file_name, data):\n",
        "    # Takes file name and saves data into a JSON file.\n",
        "    with open(file_name + \".json\", 'w', encoding='utf-8') as f:\n",
        "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
        "        print(\"The data has been exported to the JSON file named:\", file_name)\n",
        "\n",
        "# 1/2: Get the external resources for the target page and save the results a JSON file\n",
        "save_json_file(\"external_resources\", get_external_resources())\n",
        "\n",
        "# 3/4: List all links on the page, parse the words on the privacy policy page:\n",
        "privacy_policy_url = cfcurl + search_link_text(enumeration(), \"privacy policy\")\n",
        "\n",
        "save_json_file(\"word_frequency\", calculate_freq()) #Running the function to find how frequently words appear and saving the output to a JSON file."
      ]
    }
  ]
}